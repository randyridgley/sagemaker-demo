Description: " Creates resources (S3 Bucket, IAM roles, SageMaker Notebook instance) for the ML Workshop"
Parameters:

  NotebookInstanceType:
    Type: "String"
    Default: ml.t2.medium
    AllowedValues:
      - ml.t2.medium
      - ml.t2.large
      - ml.t2.xlarge
      - ml.t2.2xlarge
      - ml.m4.xlarge
      - ml.m4.2xlarge
      - ml.p2.xlarge
      - ml.p3.2xlarge
    Description: Enter a valid SageMaker notebook instance type. Defaults to ml.t2.medium.

  NotebookPrefix:
    Type: String
    Description: Prefix for the notebook instance

Resources:
  S3Bucket:
    Type: AWS::S3::Bucket

  SageMakerIamRole:
    Type: "AWS::IAM::Role"
    Properties:
      RoleName: !Sub ${NotebookPrefix}-AmazonSageMaker-ExecutionRole
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service: "sagemaker.amazonaws.com"
            Action: "sts:AssumeRole"
      Path: "/service-role/"
      ManagedPolicyArns:
        - "arn:aws:iam::aws:policy/AmazonSageMakerFullAccess"
        - "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryFullAccess"

  MLWorkshopS3Policy:
    Type: AWS::IAM::Policy
    Properties: 
      PolicyName: "S3AccessGlueAndAthena"
      PolicyDocument: 
        Version: "2012-10-17"
        Statement: 
          - 
            Effect: "Allow"
            Action: 
              - "s3:GetObject"
              - "s3:PutObject"
              - "s3:DeleteObject"
              - "s3:ListBucket"
            Resource: "arn:aws:s3:::*"
          - Effect: "Allow"
            Action: 
              - "glue:GetTables"
              - "glue:GetTable"
              - "athena:StartQueryExecution"
              - "athena:GetQueryExecution"
              - "athena:GetQueryResults"
            Resource: "*"
      Roles: 
        - Ref: SageMakerIamRole

  MLWorkshopLifecycleConfig:
    Type: "AWS::SageMaker::NotebookInstanceLifecycleConfig"
    Properties:
      OnCreate:
        - Content:
            Fn::Base64: 
                Fn::Sub: |
                  #!/bin/bash
                  set -e
                  sudo -i -u ec2-user bash << EOF
                  echo "Setup the Workshop exercises"
                  git clone https://github.com/randyridgley/sagemaker-demo.git ~/SageMaker/mlworkshop/
                  
                  cd ~/SageMaker/mlworkshop
                  
                  find -name '*.ipynb' -exec sed -i 's/<<SageMakerS3Bucket>>/${S3Bucket}/g' {} +
                  find -name '*.ipynb' -exec sed -i 's/<<GlueDatabaseName>>/${TaxiDatabase}/g' {} +
                  find -name '*.ipynb' -exec sed -i 's/<<Region>>/${AWS::Region}/g' {} +

                  git clone https://github.com/mahendrabairagi/DeeplensWorkshop.git ~/SageMaker/deeplens/
                  EOF
      OnStart:
        - Content:
            Fn::Base64: |
              #!/bin/bash
              set -e

              echo "Creating custom conda enviornment"
              cat > /home/ec2-user/mlworkshop-setup.sh << EOF
              #!/bin/bash
              set -e
              cd /home/ec2-user/SageMaker/mlworkshop
              echo "Updating base conda enviornment"
              conda update -n base conda -y
              echo "Installing mlworkshop conda env"
              conda env update
              source activate mlworkshop
              # echo "Installing AWS python libraries (sagemaker & boto3)"
              # pip install sagemaker boto3 PyAthena
              echo "Finished OnStart script"
              EOF

              chown ec2-user:ec2-user /home/ec2-user/mlworkshop-setup.sh
              chmod 755 /home/ec2-user/mlworkshop-setup.sh

              sudo -i -u ec2-user bash << EOF
              echo "Creating mlworkshop conda env in background process."
              nohup /home/ec2-user/mlworkshop-setup.sh &
              EOF

  SageMakerNotebookInstance:
    Type: "AWS::SageMaker::NotebookInstance"
    Properties:
      NotebookInstanceName: !Sub ${NotebookPrefix}-MLWorkshopInstance
      InstanceType: !Ref NotebookInstanceType
      RoleArn: !GetAtt SageMakerIamRole.Arn
      LifecycleConfigName: !GetAtt MLWorkshopLifecycleConfig.NotebookInstanceLifecycleConfigName

  TaxiDatabase:
    Type: "AWS::Glue::Database"
    Properties:
      DatabaseInput:
        Description: !Sub Database for SageMaker workshop
        Name: !Join [ '_', [ !Ref NotebookPrefix, 'taxidb' ] ]
      CatalogId: !Ref AWS::AccountId

  YellowTable:
    Type: "AWS::Glue::Table"
    Properties:
      TableInput: 
        Name: yellow
        StorageDescriptor:
          Compressed: False
          InputFormat: org.apache.hadoop.mapred.TextInputFormat 
          NumberOfBuckets: -1
          OutputFormat: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
          StoredAsSubDirectories: False 
          Location: s3://analytics-serverless-west/glue-blog/yellow
          SerdeInfo:
            SerializationLibrary: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Parameters: {'field.delim': ','}
          Columns:
          - Name: vendorid
            Type: bigint
          - Name: tpep_pickup_datetime
            Type: string
          - Name: tpep_dropoff_datetime
            Type: string
          - Name: passenger_count
            Type: double
          - Name: trip_distance
            Type: double
          - Name: pickup_longitude
            Type: double
          - Name: pickup_latitude
            Type: double
          - Name: ratecodeid
            Type: bigint
          - Name: store_and_fwd_flag
            Type: string
          - Name: dropoff_longitude
            Type: double
          - Name: dropoff_latitude
            Type: double
          - Name: payment_type
            Type: bigint
          - Name: fare_amount
            Type: double
          - Name: extra
            Type: double
          - Name: mta_tax
            Type: double
          - Name: tip_amount
            Type: double
          - Name: tolls_amount
            Type: double
          - Name: improvement_surcharge
            Type: double
          - Name: total_amount
            Type: double
        Parameters: {'classification': 'csv'} 
      DatabaseName: !Ref TaxiDatabase
      CatalogId: !Ref AWS::AccountId

  CleanupFilesFunction:
    Type: AWS::Lambda::Function
    Properties:
      Description: Lambda function to cleanup files
      Handler: index.handler
      Role: !GetAtt [LambdaExecutionRole, Arn]
      Runtime: python2.7
      Timeout: 60
      Environment:
        Variables:
          destinationbucket : !Ref S3Bucket
      Code:
        ZipFile: |
          import logging
          import json
          import zipfile
          import os
          import boto3
          from botocore.exceptions import ClientError
          from botocore.client import Config
          from urllib2 import build_opener, HTTPHandler, Request
          
          LOGGER = logging.getLogger()
          LOGGER.setLevel(logging.INFO)
          
          def send_response(event, context, response_status, response_data):
              '''Send a resource manipulation status response to CloudFormation'''
              response_body = json.dumps({
                  "Status": response_status,
                  "Reason": "See the details in CloudWatch Log Stream: " + context.log_stream_name,
                  "PhysicalResourceId": context.log_stream_name,
                  "StackId": event['StackId'],
                  "RequestId": event['RequestId'],
                  "LogicalResourceId": event['LogicalResourceId'],
                  "Data": response_data
              })
          
              LOGGER.info('ResponseURL: %s', event['ResponseURL'])
              LOGGER.info('ResponseBody: %s', response_body)
          
              opener = build_opener(HTTPHandler)
              request = Request(event['ResponseURL'], data=response_body)
              request.add_header('Content-Type', '')
              request.add_header('Content-Length', len(response_body))
              request.get_method = lambda: 'PUT'
              response = opener.open(request)
              LOGGER.info("Status code: %s", response.getcode())
              LOGGER.info("Status message: %s", response.msg)
          
          def handler(event, context):
              '''Handle Lambda event from AWS'''
              client = boto3.resource('s3')
              destinationbucket = os.environ['destinationbucket']
          
              try:
                  LOGGER.info('REQUEST RECEIVED:\n %s', event)
                  LOGGER.info('REQUEST RECEIVED:\n %s', context)
                  
                  if event['RequestType'] == 'Create':
                      LOGGER.info('CREATE!')
                      send_response(event, context, "SUCCESS",
                                    {"Message": "Resource creation successful!"})
                  elif event['RequestType'] == 'Update':
                      LOGGER.info('UPDATE!')
                      send_response(event, context, "SUCCESS",
                                    {"Message": "Resource update successful!"})
                  elif event['RequestType'] == 'Delete':
                      LOGGER.info('DELETE!')
                      bucket = client.Bucket(destinationbucket)
                      bucket.objects.all().delete()
                      send_response(event, context, "SUCCESS",
                                    {"Message": "Resource deletion successful!"})
                  else:
                      LOGGER.info('FAILED!')
                      send_response(event, context, "FAILED",
                                    {"Message": "Unexpected event received from CloudFormation"})
              except (ClientError, AttributeError) as e:
                print(e)
                LOGGER.info('FAILED!')
                send_response(event, context, "FAILED", {
                      "Message": "Exception during processing"})

  LambdaExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service:
            - lambda.amazonaws.com
          Action:
          - sts:AssumeRole
      Path: /
      Policies:
      - PolicyName: root
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action: 's3:*'
            Resource: 
              - !Join [ "", [ "arn:aws:s3:::", !Ref S3Bucket , "/*" ] ]
              - !Join [ "", [ "arn:aws:s3:::",!Ref S3Bucket ] ]
          - Effect: Allow
            Action:
            - logs:CreateLogGroup
            - logs:CreateLogStream
            - logs:PutLogEvents
            Resource: arn:aws:logs:*:*:*

  DeleteS3Files:
    Type: Custom::DeleteS3Files
    Properties:
      ServiceToken: !GetAtt [CleanupFilesFunction, Arn]

Outputs:
  SageMakerNotebookInstance:
    Value: !GetAtt SageMakerNotebookInstance.NotebookInstanceName
  IAMRole:
    Value: !GetAtt SageMakerIamRole.Arn

  GlueDatabaseName:
    Value: !Ref TaxiDatabase

  SageMakerS3Bucket:
    Value: !Ref S3Bucket
