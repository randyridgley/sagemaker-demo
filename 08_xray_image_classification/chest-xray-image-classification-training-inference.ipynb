{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image classification results on the chest x-ray dataset\n",
    "\n",
    "We used the ResNet-50 model and first trained the network with 224×224 input image size. We used data augmentation techniques such as random cropping, and image transformations. Even though a chest x-ray image is different from ImageNet images, using a pre-trained model trained on the ImageNet dataset helps in achieving better classification accuracy. Hence, we used the use_pretrained_model hyperparameter in the Amazon SageMaker image classification algorithm to train the network. Since this is a multi-label classification, we set the multi_label parameter to 1. We resized the chest x-ray images to 256 before training so that the network can crop 224×224 regions from the input image.\n",
    "\n",
    "The following code snippet shows how it can be done using the [Amazon SageMaker Estimator interface](https://sagemaker.readthedocs.io/en/latest/estimators.html) and the image classification algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'analytics-serverless-west'\n",
    "prefix = 'sagemaker/x-ray'\n",
    "\n",
    "s3train = 's3://{}/{}/train/'.format(bucket, prefix)\n",
    "print(s3train)\n",
    "\n",
    "training_image = get_image_uri(sess.boto_region_name, 'image-classification', repo_version=\"latest\")\n",
    "s3train = 's3://{}/{}/train/'.format(bucket, prefix)\n",
    "s3validation = 's3://{}/{}/validation/'.format(bucket, prefix)\n",
    "s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_ic = sagemaker.estimator.Estimator(training_image, role,\n",
    "                        train_instance_count=1,\n",
    "                        train_instance_type='ml.p3.16xlarge',\n",
    "                        train_volume_size = 50, train_max_run = 360000,\n",
    "                        input_mode= 'File', output_path=s3_output_location,\n",
    "                        sagemaker_session=sess)\n",
    "multilabel_ic.set_hyperparameters(num_layers=50, use_pretrained_model=1,\n",
    "                                        image_shape = \"3,224,224\", num_classes=14,\n",
    "                                        mini_batch_size=256, \n",
    "                                        resize=256,  epochs=100, \n",
    "                                        learning_rate=0.0005, optimizer='adam', \n",
    "                                        num_training_samples=80000,\n",
    "                                        augmentation_type = 'crop_color_transform',\n",
    "                                        precision_dtype='float32', multi_label = 1)\n",
    "train_data = sagemaker.session.s3_input(s3train, distribution='FullyReplicated',\n",
    "                                                content_type='application/x-recordio',\n",
    "                                                s3_data_type='S3Prefix')\n",
    "validation_data = sagemaker.session.s3_input(s3validation, distribution='FullyReplicated',\n",
    "                                                content_type='application/x-recordio',\n",
    "                                                s3_data_type='S3Prefix')\n",
    "data_channels = {'train': train_data, 'validation': validation_data}\n",
    "multilabel_ic.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with weighted loss\n",
    "\n",
    "An additional feature introduced in image classification is the use of weighted loss to handle class imbalance. Typically, when training with a multi-label dataset, there might be imbalance between classes. This imbalance can lead to a network leaning towards learning one class over another. To avoid that, the Amazon SageMaker image classification algorithm uses the use_weighted_loss hyperparameter to balance the samples. When this parameter is set to 1, a weight value is calculated for each label based on the number of samples of that label in the training set. First, the number of samples in each class is calculated from the training set and the weight for loss update is set to N/N_l for that class where N is the total number of samples in the training set and N_l is the total number of samples for class l in the training set. This will weigh the loss calculated for gradient update differently for each class based on their weight thereby enabling balanced training. The average AUC increased to 0.814 when trained using the weighted loss feature enabled while still using 224×224 input resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_ic = sagemaker.estimator.Estimator(training_image, role,\n",
    "                        train_instance_count=1,\n",
    "                        train_instance_type='ml.p3.16xlarge',\n",
    "                        train_volume_size = 50, train_max_run = 360000,\n",
    "                        input_mode= 'File', output_path=s3_output_location,\n",
    "                        sagemaker_session=sess)\n",
    "# multilabel_ic.set_hyperparameters(num_layers=50, use_pretrained_model=1,\n",
    "#                                         image_shape = \"3,224,224\", num_classes=14,\n",
    "#                                         mini_batch_size=256, \n",
    "#                                         resize=256,  epochs=100, \n",
    "#                                         learning_rate=0.0005, optimizer='adam', \n",
    "#                                         num_training_samples=80000,\n",
    "#                                         augmentation_type = 'crop_color_transform',\n",
    "#                                         precision_dtype='float32', multi_label = 1)\n",
    "multilabel_ic.set_hyperparameters(num_layers=50, use_pretrained_model=1,\n",
    "                                        image_shape = \"3,224,224\", num_classes=14,\n",
    "                                        mini_batch_size=256, resize=256,  epochs=100, \n",
    "                                        learning_rate=0.0005, optimizer='adam', \n",
    "                                        num_training_samples=80000, use_weighted_loss=1,\n",
    "                                        augmentation_type = 'crop_color_transform',\n",
    "                                        precision_dtype='float32', multi_label = 1)\n",
    "train_data = sagemaker.session.s3_input(s3train, distribution='FullyReplicated',\n",
    "                                                content_type='application/x-recordio',\n",
    "                                                s3_data_type='S3Prefix')\n",
    "validation_data = sagemaker.session.s3_input(s3validation, distribution='FullyReplicated',\n",
    "                                                content_type='application/x-recordio',\n",
    "                                                s3_data_type='S3Prefix')\n",
    "data_channels = {'train': train_data, 'validation': validation_data}\n",
    "multilabel_ic.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with mixed-precision\n",
    "\n",
    "The Amazon SageMaker image classification algorithm now supports training in mixed-precision mode. This is controlled by the hyperparameter, precision_dtype, which can be set to ‘float32’ (default) or ‘float16’. In mixed-precision mode, the network computes the backward and forward pass in low-precision (float16) while maintaining the master weights in high-precision (float32). This enables the training to be faster while maintaining similar accuracy. By using the mixed-precision mode, the training time was reduced by 33 percent while obtaining the overall AUC of 0.821, which is similar to the one obtained with float32 training. The training time reduction was even greater when training using two instances for the high-resolution input (see the following section) and increased to 47 percent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_ic = sagemaker.estimator.Estimator(training_image, role,\n",
    "                        train_instance_count=1,\n",
    "                        train_instance_type='ml.p3.16xlarge',\n",
    "                        train_volume_size = 50, train_max_run = 360000,\n",
    "                        input_mode= 'File', output_path=s3_output_location,\n",
    "                        sagemaker_session=sess)\n",
    "# multilabel_ic.set_hyperparameters(num_layers=50, use_pretrained_model=1,\n",
    "#                                         image_shape = \"3,224,224\", num_classes=14,\n",
    "#                                         mini_batch_size=256, \n",
    "#                                         resize=256,  epochs=100, \n",
    "#                                         learning_rate=0.0005, optimizer='adam', \n",
    "#                                         num_training_samples=80000,\n",
    "#                                         augmentation_type = 'crop_color_transform',\n",
    "#                                         precision_dtype='float32', multi_label = 1)\n",
    "multilabel_ic.set_hyperparameters(num_layers=50, use_pretrained_model=1,\n",
    "                                        image_shape = \"3,224,224\", num_classes=14,\n",
    "                                        mini_batch_size=256, resize=256, epochs=100, \n",
    "                                        learning_rate=0.0005, optimizer='adam', \n",
    "                                        num_training_samples=80000, use_weighted_loss=1, \n",
    "                                        augmentation_type = 'crop_color_transform',\n",
    "                                        precision_dtype='float16', multi_label = 1)\n",
    "\n",
    "train_data = sagemaker.session.s3_input(s3train, distribution='FullyReplicated',\n",
    "                                                content_type='application/x-recordio',\n",
    "                                                s3_data_type='S3Prefix')\n",
    "validation_data = sagemaker.session.s3_input(s3validation, distribution='FullyReplicated',\n",
    "                                                content_type='application/x-recordio',\n",
    "                                                s3_data_type='S3Prefix')\n",
    "data_channels = {'train': train_data, 'validation': validation_data}\n",
    "multilabel_ic.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with high-resolution input\n",
    "\n",
    "We then used the original input resolution by setting the image_shape parameter to 896×896. We used the use_weighted_loss feature and float32 precision for this training. We used this resolution because it allows the network to sample a 896×896 region from the 1024×1024 during data augmentation. Since the high resolution will use more memory, typically batch_size is reduced to train the network. However, because Amazon SageMaker image classification supports distributed training, we were able to maintain the batch_size by running the training across multiple instances. This is done by setting the [instance_count parameter](https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateTrainingJob.html) in the Amazon SageMaker training to 2. The average AUC for this resolution increased to 0.830, particularly for classes such as nodule, which can benefit from high-resolution input. When we trained with mixed_precision set to 1, the average AUC was 0.825. The training was done using the same code as before but setting the train_instance_count = 2, image_shape=”3,896,896” and not setting the resize parameter.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_ic = sagemaker.estimator.Estimator(training_image, role, train_instance_count=2,\n",
    "                                                train_instance_type='ml.p3.16xlarge',\n",
    "                                                train_volume_size = 50, train_max_run = 360000,\n",
    "                                                input_mode= 'File', output_path=s3_output_location,\n",
    "                                                sagemaker_session=sess)\n",
    "\n",
    "multilabel_ic.set_hyperparameters(num_layers=50, use_pretrained_model=1, \n",
    "                                        image_shape = \"3,896,896\", num_classes=14,\n",
    "                                        mini_batch_size=64, epochs=100, \n",
    "                                        learning_rate=0.00025, optimizer='adam', \n",
    "                                        num_training_samples=80000, use_weighted_loss=1, \n",
    "                                        augmentation_type = 'crop_color_transform',\n",
    "                                        precision_dtype='float32', multi_label = 1)\n",
    "\n",
    "train_data = sagemaker.session.s3_input(s3train, distribution='FullyReplicated',\n",
    "                                                content_type='application/x-recordio',\n",
    "                                                s3_data_type='S3Prefix')\n",
    "validation_data = sagemaker.session.s3_input(s3validation, distribution='FullyReplicated',\n",
    "                                                content_type='application/x-recordio',\n",
    "                                                s3_data_type='S3Prefix')\n",
    "data_channels = {'train': train_data, 'validation': validation_data}\n",
    "multilabel_ic.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Label | 224×224 | 224×224 with class balancing | 224×224 with mixed precision | 896×896 |\n",
    "|-----------| :--------------------------: | :--------------------------: | :-------: | ---------: |\n",
    "|Atelectasis | 0.772 |0.802 | 0.799 | 0.800 |\n",
    "|Cardiomegaly | 0.859 | 0.899 | 0.906 | 0.884 |\n",
    "|Effusion | 0.830 | 0.873 | 0.873 | 0.873 |\n",
    "|Infiltration | 0.626 | 0.693 | 0.691 | 0.698 |\n",
    "|Mass | 0.791 | 0.839 | 0.834 | 0.821 |\n",
    "|Nodule | 0.716 | 0.743 | 0.751 | 0.817 |\n",
    "|Pneumonia | 0.645 | 0.710 | 0.713 | 0.739 |\n",
    "|Pneumothorax | 0.778 | 0.836 | 0.862 | 0.878 |\n",
    "|Consolidation | 0.695 | 0.791 | 0.789 | 0.785 |\n",
    "|Edema | 0.799 | 0.849 | 0.863 | 0.879| \n",
    "|Emphysema | 0.850 | 0.889 | 0.909 | 0.933 |\n",
    "|Fibrosis | 0.764 | 0.791 | 0.811 | 0.822 |\n",
    "|Pleural Thickening | 0.726 | 0.758 | 0.761 | 0.785 |\n",
    "|Hernia | 0.903 | 0.929 | 0.940 | 0.911 |\n",
    "|Average AUC | 0.768 | 0.814 | 0.821 | 0.830 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the model\n",
    "\n",
    "***\n",
    "\n",
    "A trained model does nothing on its own. We now want to use the model to perform inference. For this example, that means predicting the topic mixture representing a given document.\n",
    "\n",
    "Image-classification only supports encoded .jpg and .png image formats as inference input for now. The output is the probability values for all classes encoded in JSON format, or in JSON Lines format for batch transform.\n",
    "\n",
    "This section involves several steps,\n",
    "\n",
    "1. [Create Model](#CreateModel) - Create model for the training output\n",
    "1. [Batch Transform](#BatchTransform) - Create a transform job to perform batch inference.\n",
    "1. [Host the model for realtime inference](#HostTheModel) - Create an inference endpoint and perform realtime inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model\n",
    "\n",
    "We now create a SageMaker Model from the training output. Using the model we can create a Batch Transform Job or an Endpoint Configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import boto3\n",
    "from time import gmtime, strftime\n",
    "\n",
    "sage = boto3.Session().client(service_name='sagemaker') \n",
    "\n",
    "# get the name of the training job completed below for this variable\n",
    "job_name=\"image-classification-2018-11-03-07-10-36-441\"\n",
    "model_name = \"x-ray-image-classification-model\"\n",
    "print(model_name)\n",
    "info = sage.describe_training_job(TrainingJobName=job_name)\n",
    "model_data = info['ModelArtifacts']['S3ModelArtifacts']\n",
    "print(model_data)\n",
    "\n",
    "hosting_image = get_image_uri(boto3.Session().region_name, 'image-classification')\n",
    "\n",
    "primary_container = {\n",
    "    'Image': hosting_image,\n",
    "    'ModelDataUrl': model_data,\n",
    "}\n",
    "\n",
    "create_model_response = sage.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    PrimaryContainer = primary_container)\n",
    "\n",
    "print(create_model_response['ModelArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realtime inference\n",
    "\n",
    "We now host the model with an endpoint and perform realtime inference.\n",
    "\n",
    "This section involves several steps,\n",
    "1. [Create endpoint configuration](#CreateEndpointConfiguration) - Create a configuration defining an endpoint.\n",
    "1. [Create endpoint](#CreateEndpoint) - Use the configuration to create an inference endpoint.\n",
    "1. [Perform inference](#PerformInference) - Perform inference on some input data using the endpoint.\n",
    "1. [Clean up](#CleanUp) - Delete the endpoint and model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Endpoint Configuration\n",
    "At launch, we will support configuring REST endpoints in hosting with multiple models, e.g. for A/B testing purposes. In order to support this, customers create an endpoint configuration, that describes the distribution of traffic across the models, whether split, shadowed, or sampled in some way.\n",
    "\n",
    "In addition, the endpoint configuration describes the instance type required for model deployment, and at launch will describe the autoscaling configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from time import gmtime, strftime\n",
    "\n",
    "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "endpoint_config_name = model_name + '-epc-' + timestamp\n",
    "endpoint_config_response = sage.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType':'ml.m4.xlarge',\n",
    "        'InitialInstanceCount':1,\n",
    "        'ModelName':model_name,\n",
    "        'VariantName':'AllTraffic'}])\n",
    "\n",
    "print('Endpoint configuration name: {}'.format(endpoint_config_name))\n",
    "print('Endpoint configuration arn:  {}'.format(endpoint_config_response['EndpointConfigArn']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Endpoint\n",
    "Lastly, the customer creates the endpoint that serves up the model, through specifying the name and configuration defined above. The end result is an endpoint that can be validated and incorporated into production applications. This takes 9-11 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "\n",
    "sage = boto3.client('sagemaker')\n",
    "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "endpoint_name = model_name + '-ep-' + timestamp\n",
    "print('Endpoint name: {}'.format(endpoint_name))\n",
    "\n",
    "endpoint_params = {\n",
    "    'EndpointName': endpoint_name,\n",
    "    'EndpointConfigName': endpoint_config_name,\n",
    "}\n",
    "endpoint_response = sage.create_endpoint(**endpoint_params)\n",
    "print('EndpointArn = {}'.format(endpoint_response['EndpointArn']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the status of the endpoint\n",
    "response = sage.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = response['EndpointStatus']\n",
    "print('EndpointStatus = {}'.format(status))\n",
    "\n",
    "\n",
    "# wait until the status has changed\n",
    "sage.get_waiter('endpoint_in_service').wait(EndpointName=endpoint_name)\n",
    "\n",
    "\n",
    "# print the status of the endpoint\n",
    "endpoint_response = sage.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = endpoint_response['EndpointStatus']\n",
    "print('Endpoint creation ended with EndpointStatus = {}'.format(status))\n",
    "\n",
    "if status != 'InService':\n",
    "    raise Exception('Endpoint creation failed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see the message,\n",
    "\n",
    "> `Endpoint creation ended with EndpointStatus = InService`\n",
    "\n",
    "then congratulations! You now have a functioning inference endpoint. You can confirm the endpoint configuration and status by navigating to the \"Endpoints\" tab in the AWS SageMaker console.\n",
    "\n",
    "We will finally create a runtime object from which we can invoke the endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform Inference\n",
    "Finally, the customer can now validate the model for use. They can obtain the endpoint from the client library using the result from previous operations, and generate classifications from the trained model using that endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "runtime = boto3.Session().client(service_name='runtime.sagemaker') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -10 chestxraytest.lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '/home/ec2-user/SageMaker/images/00000003_002.png'\n",
    "# test image\n",
    "from IPython.display import Image\n",
    "Image(file_name)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "with open(file_name, 'rb') as f:\n",
    "    payload = f.read()\n",
    "    payload = bytearray(payload)\n",
    "endpoint_name = 'x-ray-image-classification-model-ep--2018-11-03-16-26-35'\n",
    "response = runtime.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                   ContentType='application/x-image', \n",
    "                                   Body=payload)\n",
    "result = response['Body'].read()\n",
    "# result will be in json format and convert it to ndarray\n",
    "result = json.loads(result)\n",
    "print(result)\n",
    "# the result will output the probabilities for all classes\n",
    "# find the class with maximum probability and print the class index\n",
    "index = np.argmax(result)\n",
    "print(index)\n",
    "disease_list = ['Atelectasis', 'Consolidation', 'Infiltration', 'Pneumothorax', 'Edema', 'Emphysema', \\\n",
    "                   'Fibrosis', 'Effusion', 'Pneumonia', 'Pleural_Thickening', 'Cardiomegaly', 'Nodule', 'Mass', \\\n",
    "                   'Hernia']\n",
    "\n",
    "\n",
    "print(\"Top Result: label - \" + disease_list[index] + \", probability - \" + str(result[index]))\n",
    "for idx, val in enumerate(result):\n",
    "    print('%s:\\t%f \\n'%(disease_list[idx], result[idx]), end='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
